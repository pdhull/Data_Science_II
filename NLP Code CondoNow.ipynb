{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                                        Data Science II\n",
    "                                                        Group Project 6\n",
    "                                              Natural Language Processing Pseudo Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries and Sample Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first imported the basic libraries to the Jupiter Notebook which will help in the analysis and deciphering results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The command pip install pandas spacy is a concise way to install two essential Python libraries. Firstly, pandas is a versatile data manipulation and analysis library, offering powerful data structures like DataFrames for efficient handling of structured data. Secondly, spacy is a natural language processing (NLP) library that facilitates tasks such as tokenization, part-of-speech tagging, and named entity recognition, making it valuable for text analysis. Together, these libraries enable users to work with structured data and perform sophisticated text processing in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (2.0.1)\n",
      "Requirement already satisfied: spacy in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (3.7.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/pratiksha/Library/Python/3.11/lib/python/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pandas) (1.24.3)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from spacy) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from spacy) (8.2.1)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from spacy) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from spacy) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from spacy) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from spacy) (0.9.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from spacy) (6.4.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from spacy) (4.66.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from spacy) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from spacy) (2.5.2)\n",
      "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from spacy) (65.5.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/pratiksha/Library/Python/3.11/lib/python/site-packages (from spacy) (23.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.14.5)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.7.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/pratiksha/Library/Python/3.11/lib/python/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.5.7)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.1.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from jinja2->spacy) (2.1.3)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The command pip install nltk is used to install the Natural Language Toolkit (NLTK) library in Python. NLTK is a powerful library for natural language processing (NLP) that provides tools and resources for tasks such as tokenization, stemming, lemmatization, part-of-speech tagging, and more. It is widely used for processing and analyzing human language data in various applications, including text mining, information retrieval, and machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (3.8.1)\n",
      "Requirement already satisfied: click in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from nltk) (4.66.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The provided code uses the Natural Language Toolkit (NLTK) in Python to download essential resources. The command nltk.download('punkt') fetches the Punkt tokenizer models, which are pre-trained models for text tokenization, breaking text into individual words or sentences. Tokenization is a fundamental step in natural language processing. The second command, nltk.download('stopwords'), retrieves a list of common stop words in various languages. Stop words, like \"and\" or \"the,\" are frequently used in language but are often disregarded in text analysis since they lack significant semantic meaning. Downloading this list enables the exclusion of stop words during text processing, allowing a focus on more meaningful content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/pratiksha/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/pratiksha/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This line of code imports the stopwords module from the Natural Language Toolkit (NLTK) corpus. The stopwords module contains a collection of common stop words for various languages. Stop words are words that are frequently used in a language but are often ignored in text processing tasks because they don't carry significant meaning and can be considered noise in certain analyses. By importing the stopwords module, you gain access to lists of stop words that can be used to filter out these common words from text data during text analysis or natural language processing tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the Questions Dataset from your local file and store it in df. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.read_csv('/Users/pratiksha/Desktop/Book1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We printed the dataframe df to check how it looks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Questions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Are there pre-construction homes with easy access to public transit in Brantford?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How close are the pre-construction homes in Orillia to major highways?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What are the typical ceiling heights in pre-construction homes in Woodstock?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Can I find pre-construction homes with large windows in Aurora?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Are there environmentally friendly pre-construction homes in Stratford?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>868</th>\n",
       "      <td>proximity to amenities, 3Bedroom, Oakville</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>869</th>\n",
       "      <td>occupancy date, TTC, Milton</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>870</th>\n",
       "      <td>pet friendly, crime rates, Whitby</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>871</th>\n",
       "      <td>payment structure, size, Toronto</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>872</th>\n",
       "      <td>location, proximity to amenities, Brampton</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>873 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                             Questions\n",
       "0    Are there pre-construction homes with easy access to public transit in Brantford?\n",
       "1               How close are the pre-construction homes in Orillia to major highways?\n",
       "2         What are the typical ceiling heights in pre-construction homes in Woodstock?\n",
       "3                      Can I find pre-construction homes with large windows in Aurora?\n",
       "4              Are there environmentally friendly pre-construction homes in Stratford?\n",
       "..                                                                                 ...\n",
       "868                                         proximity to amenities, 3Bedroom, Oakville\n",
       "869                                                        occupancy date, TTC, Milton\n",
       "870                                                  pet friendly, crime rates, Whitby\n",
       "871                                                   payment structure, size, Toronto\n",
       "872                                         location, proximity to amenities, Brampton\n",
       "\n",
       "[873 rows x 1 columns]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "________________________________________________________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this Python code snippet below, the spaCy library is employed for natural language processing. Initially, the script downloads the spaCy English language model, \"en_core_web_sm,\" which encompasses pre-trained components for various linguistic tasks. Subsequently, the model is loaded into a spaCy pipeline. A function, convert_to_lowercase, is defined to utilize spaCy for tokenization and conversion of each token to lowercase. This function is then applied to the \"Questions\" column of a DataFrame (df), leveraging the spaCy pipeline to convert the text in this column to lowercase. Finally, the updated DataFrame is displayed, showcasing the effects of the text transformation. This process is integral for maintaining consistency and simplifying subsequent text-based analyses or natural language processing tasks. This is basically done to change the words to lowercase if a few alphabets were in upper case in the questions dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.7.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12.8/12.8 MB 730.3 kB/s eta 0:00:00\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from en-core-web-sm==3.7.1) (3.7.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.1)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (6.4.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.5.2)\n",
      "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (65.5.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/pratiksha/Library/Python/3.11/lib/python/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (23.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.24.3)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.14.5)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2023.5.7)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 23.3.1\n",
      "[notice] To update, run: python3 -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "                                                                                Questions\n",
      "0    are there pre - construction homes with easy access to public transit in brantford ?\n",
      "1               how close are the pre - construction homes in orillia to major highways ?\n",
      "2         what are the typical ceiling heights in pre - construction homes in woodstock ?\n",
      "3                      can i find pre - construction homes with large windows in aurora ?\n",
      "4              are there environmentally friendly pre - construction homes in stratford ?\n",
      "..                                                                                    ...\n",
      "868                                          proximity to amenities , 3bedroom , oakville\n",
      "869                                                         occupancy date , ttc , milton\n",
      "870                                                   pet friendly , crime rates , whitby\n",
      "871                                                    payment structure , size , toronto\n",
      "872                                          location , proximity to amenities , brampton\n",
      "\n",
      "[873 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Download spaCy English language model\n",
    "spacy.cli.download(\"en_core_web_sm\")\n",
    "\n",
    "# Load the downloaded model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Function to convert text to lowercase using spaCy\n",
    "def convert_to_lowercase(text):\n",
    "    doc = nlp(text)\n",
    "    return ' '.join([token.text.lower() for token in doc])\n",
    "\n",
    "# Apply the function to the \"Questions\" column\n",
    "df['Questions'] = df['Questions'].apply(convert_to_lowercase)\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this Python code snippet below, the string module is imported, and a function named remove_punctuation is defined. This function takes a text input and utilizes a list comprehension to exclude any characters that are punctuation marks according to the string.punctuation set. The remove_punctuation function is then applied to the \"Questions\" column of a DataFrame (df). This process effectively removes all punctuation from the text in that column. Finally, the updated DataFrame is displayed, illustrating the impact of the punctuation removal. This preprocessing step is valuable for text analysis, as it helps ensure consistency and eliminates noise from the textual data, making it more suitable for subsequent natural language processing tasks or analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                              Questions\n",
      "0    are there pre  construction homes with easy access to public transit in brantford \n",
      "1               how close are the pre  construction homes in orillia to major highways \n",
      "2         what are the typical ceiling heights in pre  construction homes in woodstock \n",
      "3                      can i find pre  construction homes with large windows in aurora \n",
      "4              are there environmentally friendly pre  construction homes in stratford \n",
      "..                                                                                  ...\n",
      "868                                          proximity to amenities  3bedroom  oakville\n",
      "869                                                         occupancy date  ttc  milton\n",
      "870                                                   pet friendly  crime rates  whitby\n",
      "871                                                    payment structure  size  toronto\n",
      "872                                          location  proximity to amenities  brampton\n",
      "\n",
      "[873 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "# Function to remove punctuation from text\n",
    "def remove_punctuation(text):\n",
    "    return ''.join([char for char in text if char not in string.punctuation])\n",
    "\n",
    "# Apply the function to the \"Questions\" column\n",
    "df['Questions'] = df['Questions'].apply(remove_punctuation)\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The line of code below uses the Natural Language Toolkit (NLTK) library to create a set named stop_words containing common English stop words. By creating a set of these stop words, it becomes convenient to later filter out these words from text data during natural language processing tasks, contributing to more meaningful analyses by focusing on content-carrying words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We check what the stop words look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Python code snippet below, a function named remove_stopwords is defined to remove common English stop words from text. The function uses the NLTK library's word_tokenize method to tokenize the input text into individual words. It then creates a filtered list of words by excluding those that are found in the set of stop words (stop_words). The filtered words are then joined back into a string, effectively removing the stop words from the original text. This function is applied to the \"Questions\" column of a DataFrame (df), and the updated DataFrame is displayed. The removal of stop words is a crucial preprocessing step in text analysis, as it helps focus on the more informative words in the text and contributes to improved accuracy in natural language processing tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                       Questions\n",
      "0    pre construction homes easy access public transit brantford\n",
      "1            close pre construction homes orillia major highways\n",
      "2       typical ceiling heights pre construction homes woodstock\n",
      "3               find pre construction homes large windows aurora\n",
      "4      environmentally friendly pre construction homes stratford\n",
      "..                                                           ...\n",
      "868                        proximity amenities 3bedroom oakville\n",
      "869                                    occupancy date ttc milton\n",
      "870                              pet friendly crime rates whitby\n",
      "871                               payment structure size toronto\n",
      "872                        location proximity amenities brampton\n",
      "\n",
      "[873 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "# Function to remove stopwords from text\n",
    "def remove_stopwords(text):\n",
    "    word_tokens = word_tokenize(text)\n",
    "    filtered_text = [word for word in word_tokens if word.lower() not in stop_words]\n",
    "    return ' '.join(filtered_text)\n",
    "\n",
    "# Apply the function to the \"Questions\" column\n",
    "df['Questions'] = df['Questions'].apply(remove_stopwords)\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code snippet below, the pd.set_option('display.max_colwidth', None) line sets the display option for pandas to show the full content of cells in a DataFrame, ensuring that the entire text content of each cell is visible. This is particularly useful when dealing with text data or long strings in DataFrame columns. Following this, the DataFrame (df) is printed, displaying the full content of the cells in the \"Questions\" column without any truncation. This adjustment in display options allows for a comprehensive view of the text data within the DataFrame, facilitating a more detailed inspection of the information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                       Questions\n",
      "0    pre construction homes easy access public transit brantford\n",
      "1            close pre construction homes orillia major highways\n",
      "2       typical ceiling heights pre construction homes woodstock\n",
      "3               find pre construction homes large windows aurora\n",
      "4      environmentally friendly pre construction homes stratford\n",
      "..                                                           ...\n",
      "868                        proximity amenities 3bedroom oakville\n",
      "869                                    occupancy date ttc milton\n",
      "870                              pet friendly crime rates whitby\n",
      "871                               payment structure size toronto\n",
      "872                        location proximity amenities brampton\n",
      "\n",
      "[873 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "# Set display option to show the full content of cells\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now printed the first twenty rows of the dataframe to have an idea how the keywords look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Questions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pre construction homes easy access public transit brantford</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>close pre construction homes orillia major highways</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>typical ceiling heights pre construction homes woodstock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>find pre construction homes large windows aurora</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>environmentally friendly pre construction homes stratford</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>deposit structure pre construction homes north bay</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>pre construction townhomes available newmarket</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>find pre construction homes open concept layout cornwall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>warranty options pre construction homes georgetown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>many floors pre construction homes cobourg typically</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>pre construction homes gym facility lethbridge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>pre construction homes orangeville offer landscaped gardens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>kind community amenities available pre construction homes grimsby</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>pre construction homes home office space brockville</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>pre construction homes timmins designed accessibility</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>average balcony size pre construction condos owen sound</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>pre construction homes children play area huntsville</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>find pre construction homes pool collingwood</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>pre construction homes high speed internet infrastructure fort erie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>common features pre construction homes midland</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                              Questions\n",
       "0           pre construction homes easy access public transit brantford\n",
       "1                   close pre construction homes orillia major highways\n",
       "2              typical ceiling heights pre construction homes woodstock\n",
       "3                      find pre construction homes large windows aurora\n",
       "4             environmentally friendly pre construction homes stratford\n",
       "5                    deposit structure pre construction homes north bay\n",
       "6                        pre construction townhomes available newmarket\n",
       "7              find pre construction homes open concept layout cornwall\n",
       "8                    warranty options pre construction homes georgetown\n",
       "9                  many floors pre construction homes cobourg typically\n",
       "10                       pre construction homes gym facility lethbridge\n",
       "11          pre construction homes orangeville offer landscaped gardens\n",
       "12    kind community amenities available pre construction homes grimsby\n",
       "13                  pre construction homes home office space brockville\n",
       "14                pre construction homes timmins designed accessibility\n",
       "15              average balcony size pre construction condos owen sound\n",
       "16                 pre construction homes children play area huntsville\n",
       "17                         find pre construction homes pool collingwood\n",
       "18  pre construction homes high speed internet infrastructure fort erie\n",
       "19                       common features pre construction homes midland"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "________________________________________________________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample of NLP Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1: Defining Questions**\n",
    "\n",
    "The model takes in a list of sample questions related to finding apartments or houses. Each question contains details like the number of bedrooms, bathrooms, parking requirements, and the expected price.\n",
    "\n",
    "**Step 2: Converting Words to Numbers**\n",
    "\n",
    "There's a function called words_to_numbers that takes a text input and converts words representing numbers (e.g., 'one', 'two') into actual numeric values. This is done to standardize the representation of numbers in the questions.\n",
    "\n",
    "**Step 3: Extracting Information**\n",
    "\n",
    "The main function, extract_information, is designed to extract specific details from each question. It looks for patterns in the text using regular expressions to identify the number of bedrooms, bathrooms, the price, and whether parking is mentioned. Additionally, it uses a simple approach to recognize the location mentioned in the question.\n",
    "\n",
    "**Step 4: Iterating Through Questions**\n",
    "\n",
    "The code then goes through each question in the list and applies the extract_information function. The results are stored in a list.\n",
    "\n",
    "**Step 5: Creating a DataFrame**\n",
    "\n",
    "The results are then used to create a DataFrame, a structured tabular representation of the extracted information.\n",
    "\n",
    "**Step 6: Displaying the Resulting DataFrame**\n",
    "\n",
    "Finally, the resulting DataFrame is displayed. This DataFrame organizes the extracted details, such as location, number of bedrooms and bathrooms, price, and parking information, making it easier to analyze and understand the key features mentioned in the questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Location  Bedroom  Bathroom    Price Parking\n",
      "0       Toronto        2         1  8000000     yes\n",
      "1      Waterloo        3         2  3000000    None\n",
      "2          York        3         1  5000000     yes\n",
      "3      Hamilton        1         1  1000000    None\n",
      "4       Toronto        2         1  6000000     yes\n",
      "5   Mississauga        2         1  2000000    None\n",
      "6        London        3         2  4000000     yes\n",
      "7        Ottawa        1         1  1000000     yes\n",
      "8       Toronto        2         2  7000000     yes\n",
      "9      Montreal        1         1  1000000    None\n",
      "10      Vaughan        2         1  5000000     yes\n",
      "11     Oakville        2         1  2000000    None\n",
      "12   North York        3         2  8000000     yes\n",
      "13     Waterloo        2         1  2000000     yes\n",
      "14   Burlington        3         2  1000000     yes\n",
      "15      Markham        2         1  7000000    None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# List of example questions\n",
    "questions = [\n",
    "    \"I'm looking for a two bedroom and one bathroom apartment in Toronto with parking for $8000000\",\n",
    "    \"Looking for a three bedroom, two bathroom house in Waterloo for $3000000\",\n",
    "    \"Need a three bedroom and one bathroom apartment in York with parking for around $5000000\",\n",
    "    \"Need a one bedroom and one bathroom apartment in Hamilton for around $1000000\",\n",
    "    \"I'm looking for a two bedroom and one bathroom apartment in Toronto with parking for $6000000\",\n",
    "    \"Looking for a two bedroom, one bathroom house in Mississauga for $2000000\",\n",
    "    \"Need a three bedroom and two bathroom apartment in London with parking for around $4000000\",\n",
    "    \"Need a one bedroom and one bathroom apartment in Ottawa with parking for around $1000000\",\n",
    "    \"I'm looking for a two bedroom and two bathroom apartment in Toronto with parking for $7000000\",\n",
    "    \"Looking for a one bedroom, one bathroom house in Montreal for $1000000\",\n",
    "    \"Need a two bedroom and one bathroom apartment in Vaughan with parking for around $5000000\",\n",
    "    \"Need a two bedroom and one bathroom apartment in Oakville for around $2000000\",\n",
    "    \"I'm looking for a three bedroom and two bathroom apartment in North York with parking for $8000000\",\n",
    "    \"Looking for a two bedroom, one bathroom house in Waterloo with parking for $2000000\",\n",
    "    \"Need a three bedroom and two bathroom apartment in Burlington with parking for around $1000000\",\n",
    "    \"Need a two bedroom and one bathroom apartment in Markham for around $7000000\"\n",
    "]\n",
    "\n",
    "# Function to convert words to numbers\n",
    "def words_to_numbers(text):\n",
    "    word_to_number = {\n",
    "        'one': 1,\n",
    "        'two': 2,\n",
    "        'three': 3,\n",
    "        # Add more mappings as needed\n",
    "    }\n",
    "\n",
    "    for word, number in word_to_number.items():\n",
    "        text = re.sub(fr'\\b{word}\\b', str(number), text, flags=re.IGNORECASE)\n",
    "\n",
    "    return text\n",
    "\n",
    "# Function to extract relevant information from the question\n",
    "def extract_information(text):\n",
    "    location = None\n",
    "    bedroom = None\n",
    "    bathroom = None\n",
    "    price = None\n",
    "    parking = None\n",
    "\n",
    "    text = words_to_numbers(text)\n",
    "\n",
    "    # Extract information using regular expressions\n",
    "    bedroom_match = re.search(r'(\\d+)\\s*bedroom', text, re.IGNORECASE)\n",
    "    bathroom_match = re.search(r'(\\d+)\\s*bathroom', text, re.IGNORECASE)\n",
    "    price_match = re.search(r'\\$([\\d,]+)', text)\n",
    "    parking_match = re.search(r'\\bparking\\b', text, re.IGNORECASE)\n",
    "\n",
    "    # Extract location using named entity recognition\n",
    "    # Extract location using named entity recognition\n",
    "    locations = ['Toronto', 'Waterloo', 'York', 'Hamilton', 'Mississauga', 'London', 'Ottawa', 'Vaughan', 'Oakville', 'North York', 'Montreal', 'Burlington', 'Markham']\n",
    "    for loc in locations:\n",
    "        if loc.lower() in text.lower():\n",
    "            location = loc\n",
    "\n",
    "    # Extract values if matches are found\n",
    "    if bedroom_match:\n",
    "        bedroom = int(bedroom_match.group(1))\n",
    "    if bathroom_match:\n",
    "        bathroom = int(bathroom_match.group(1))\n",
    "    if price_match:\n",
    "        price = price_match.group(1)\n",
    "    if parking_match:\n",
    "        parking = 'yes'\n",
    "\n",
    "    result = {\n",
    "        'Location': location,\n",
    "        'Bedroom': bedroom,\n",
    "        'Bathroom': bathroom,\n",
    "        'Price': price,\n",
    "        'Parking': parking\n",
    "    }\n",
    "\n",
    "    return result\n",
    "\n",
    "# Extract information from each question and store the results in a list\n",
    "results_list = [extract_information(question) for question in questions]\n",
    "\n",
    "# Create a DataFrame from the list of results\n",
    "df_result = pd.DataFrame(results_list)\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "print(df_result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data dictionary, obtained from user queries to the chatbot, forms the backbone of our NLP Model. By encapsulating various keywords extracted from user questions, this dictionary becomes a powerful tool for efficiently filtering personalized condo recommendations. Users no longer need to manually navigate around 20 filters on the CondoNow website; a simple query to the chatbot, outlining their requirements, initiates the generation of a tailored list of condo listings. This streamlined process saves users time and effort as the chatbot serves as an alternative to manual filtering. Once presented with a curated list of 5-7 properties aligning with their preferences, users can conveniently share their contact details with the chatbot. This information is seamlessly integrated into CondoNow's CRM software, facilitating a swift connection between the interested user and a dedicated agent. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "________________________________________________________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation and Accuracy of the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to effectively evaluate and validate the model we can employ any number of techniques. These techniques would work to reduce the overall bias and ensure the integrity of the model is kept. These methods include: \n",
    "\n",
    "- Train-Test Split with Text Data:\n",
    "\n",
    "Text Data Splitting: Split your dataset into training and validation/test sets, ensuring that the text distribution is maintained in both sets.\n",
    "Metrics: Use NLP-specific metrics such as accuracy, precision, recall, F1 score, and confusion matrices for classification tasks. For regression tasks, consider metrics like mean squared error.\n",
    "\n",
    "- Cross-Validation for Text Data:\n",
    "\n",
    "Stratified K-Fold Cross-Validation: Adapt cross-validation techniques to account for the unique nature of text data. Ensure that each fold maintains a similar distribution of text patterns.\n",
    "\n",
    "- Tokenization and Padding:\n",
    "\n",
    "Tokenization: Ensure that your text is tokenized appropriately, breaking it down into meaningful units (words, subwords, or characters).\n",
    "Padding: If using deep learning models, consider padding sequences to a consistent length to enable batch processing.\n",
    "\n",
    "- Embeddings and Word Vectors:\n",
    "\n",
    "Word Embeddings: If applicable, leverage pre-trained word embeddings (Word2Vec, GloVe, FastText) to capture semantic relationships between words.\n",
    "Embedding Layers: When using deep learning models, include embedding layers to learn embeddings specific to your task.\n",
    "\n",
    "- Sequence Models:\n",
    "\n",
    "Recurrent Neural Networks (RNNs) and LSTMs: For sequence-based tasks, consider using RNNs or LSTMs to capture sequential dependencies in the text.\n",
    "Attention Mechanisms: Explore attention mechanisms for tasks requiring a focus on specific parts of the input sequence.\n",
    "\n",
    "- NLP-Specific Metrics:\n",
    "\n",
    "BLEU Score: For machine translation tasks, BLEU (Bilingual Evaluation Understudy) measures the quality of the generated text compared to a reference translation.\n",
    "ROUGE Score: Used for summarization tasks, ROUGE evaluates the overlap between model-generated summaries and human-generated references.\n",
    "\n",
    "- BERT and Transformers:\n",
    "\n",
    "Fine-Tuning BERT: If your task involves understanding contextual relationships in text, consider fine-tuning pre-trained transformer models like BERT (Bidirectional Encoder Representations from Transformers).\n",
    "\n",
    "**Model Explainability:**\n",
    "\n",
    "- Interpretability in NLP: Utilize techniques for model interpretability, such as LIME (Local Interpretable Model-Agnostic Explanations) or SHAP values, to explain individual predictions.\n",
    "\n",
    "- Domain-Specific Evaluation:\n",
    "Domain-Specific Metrics: Define task-specific metrics if standard metrics don't capture the nuances of your NLP problem.\n",
    "\n",
    "- Bias and Fairness:\n",
    "Addressing Bias: Be aware of bias in your model, especially in NLP where biases in training data can impact the model's behavior. Evaluate and mitigate bias where possible.\n",
    "\n",
    "The choice of techniques depends on the specific NLP task which will be done as the model progresses and is trained on the actual data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "________________________________________________________________________________________________________________________________________________"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
